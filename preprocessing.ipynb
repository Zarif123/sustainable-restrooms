{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "import soundfile as sf\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import math, random\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training_set\\urination\\urination_1_0_quant_3.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training_set\\urination\\urination_1_10_quant_3.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training_set\\urination\\urination_1_11_quant_3.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training_set\\urination\\urination_1_12_quant_3.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training_set\\urination\\urination_1_13_quant_3.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       relative_path  class_id\n",
       "0   training_set\\urination\\urination_1_0_quant_3.wav         2\n",
       "1  training_set\\urination\\urination_1_10_quant_3.wav         2\n",
       "2  training_set\\urination\\urination_1_11_quant_3.wav         2\n",
       "3  training_set\\urination\\urination_1_12_quant_3.wav         2\n",
       "4  training_set\\urination\\urination_1_13_quant_3.wav         2"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_folder = 'training_set'\n",
    "\n",
    "training_path = Path.cwd()/training_folder\n",
    "\n",
    "num_classes = 4\n",
    "train_classes = {'urination': 2, 'defecation': 3, 'silence': 0}\n",
    "training_data_dict = {'relative_path': [], 'class_id': []}\n",
    "\n",
    "for class_name in train_classes:\n",
    "    path = f'{training_path}/{class_name}'\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.wav'):\n",
    "            training_data_dict['relative_path'].append(os.path.join(f'{training_folder}\\\\{class_name}', file))\n",
    "            training_data_dict['class_id'].append(train_classes[class_name])\n",
    "                \n",
    "training_df = pd.DataFrame(training_data_dict)\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_dataset = load_dataset(\"CAiRE/ASCEND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1003)\n",
    "\n",
    "speech_dict = {'relative_path': [], 'class_id': []}\n",
    "\n",
    "for i in range(80):\n",
    "    speech_dict['relative_path'].append(random.randint(0, len(speech_dataset['train']) - 1))\n",
    "    speech_dict['class_id'].append(1)\n",
    "\n",
    "speech_df = pd.DataFrame(speech_dict)\n",
    "training_df = pd.concat([training_df, speech_df])\n",
    "training_df = training_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil():\n",
    "  @staticmethod\n",
    "  def open(audio_file, class_id, testing):\n",
    "    if class_id != 1 or testing == True and class_id == 1:\n",
    "      sig, sr = torchaudio.load(audio_file) #signal and sample rate\n",
    "    else:\n",
    "      #speech data\n",
    "      sig, sr = torch.tensor(speech_dataset['train'][audio_file]['audio']['array']), speech_dataset['train'][audio_file]['audio']['sampling_rate']\n",
    "      sig = sig.view(1, sig.shape[0]).to(torch.float32)\n",
    "    return (sig, sr)\n",
    "  \n",
    "  @staticmethod\n",
    "  def plot_waveform(waveform, sample_rate):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "    figure.suptitle(\"waveform\")\n",
    "\n",
    "  @staticmethod\n",
    "  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return (spec)\n",
    "  \n",
    "  @staticmethod\n",
    "  def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)\n",
    "  \n",
    "  @staticmethod\n",
    "  def resample(aud, newsr):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sr == newsr):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    num_channels = sig.shape[0]\n",
    "    # Resample first channel\n",
    "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "    if (num_channels > 1):\n",
    "      # Resample the second channel and merge both channels\n",
    "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "      resig = torch.cat([resig, retwo])\n",
    "\n",
    "    return ((resig, newsr))\n",
    "  \n",
    "  @staticmethod\n",
    "  def rechannel(aud, new_channel):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sig.shape[0] == new_channel):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    if (new_channel == 1):\n",
    "      # Convert from stereo to mono by selecting only the first channel\n",
    "      resig = sig[:1, :]\n",
    "    else:\n",
    "      # Convert from mono to stereo by duplicating the first channel\n",
    "      resig = torch.cat([sig, sig])\n",
    "\n",
    "    return ((resig, sr))\n",
    "  \n",
    "  @staticmethod\n",
    "  def time_shift(aud, shift_limit):\n",
    "    sig,sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    return (sig.roll(shift_amt), sr)\n",
    "  \n",
    "  @staticmethod\n",
    "  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataSet(Dataset):\n",
    "  def __init__(self, df, testing):\n",
    "    self.df = df\n",
    "    self.duration = 4000\n",
    "    self.channel = 2\n",
    "    self.sr = 48000\n",
    "    self.shift_pct = 0.4\n",
    "    self.testing = testing\n",
    "            \n",
    "  def __len__(self):\n",
    "    return len(self.df)    \n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    audio_file = self.df.loc[idx, 'relative_path']\n",
    "    class_id = self.df.loc[idx, 'class_id']\n",
    "\n",
    "    aud = AudioUtil.open(audio_file, class_id, self.testing)\n",
    "    reaud = AudioUtil.resample(aud, self.sr)\n",
    "    rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "\n",
    "    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "    return aug_sgram, class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = torch.nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.bn1 = torch.nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Convolution Block\n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.bn2 = torch.nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Convolution Block\n",
    "        self.conv3 = torch.nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        self.bn3 = torch.nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Convolution Block\n",
    "        self.conv4 = torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "        self.bn4 = torch.nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = torch.nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = torch.nn.Linear(in_features=64, out_features=num_classes)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = torch.nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_dl, num_epochs):\n",
    "  # Loss Function, Optimizer and Scheduler\n",
    "  criterion = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "  # Repeat for each epoch\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Repeat for each batch in the training set\n",
    "    for i, data in enumerate(train_dl):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Normalize the inputs\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Keep stats for Loss and Accuracy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Get the predicted class with the highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        # Count of predictions that matched the target label\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "\n",
    "        #if i % 10 == 0:    # print every 10 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "    \n",
    "    # Print stats at the end of the epoch\n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    if epoch % 25 == 0:\n",
    "       print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "  print(f'Finished Training, total items {total_prediction}')\n",
    "\n",
    "def testing(model, val_dl, validation):\n",
    "    classes_map = {2: 'urination', 3: 'defecation', 0: 'silence', 1: 'speech'}\n",
    "    prediction_list = []\n",
    "    weighted_average = 0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    iter_count = 0\n",
    "\n",
    "    # Disable gradient updates\n",
    "    with torch.no_grad():\n",
    "      for data in val_dl:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Normalization\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Get predictions\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get class with highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        if validation:\n",
    "           correct_prediction += (prediction == labels).sum().item()\n",
    "           total_prediction += prediction.shape[0]\n",
    "           acc = correct_prediction/total_prediction\n",
    "           print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "        else:\n",
    "          #  weighted_average += sum(prediction.tolist()) / prediction.shape[0]\n",
    "           prediction_list = [classes_map[pred] for pred in prediction.tolist()]\n",
    "           print(f'Predictions for {len(prediction_list)} seconds', prediction_list)\n",
    "        iter_count += 1\n",
    "\n",
    "      # if not validation:\n",
    "      #    final_class = math.ceil(weighted_average / iter_count)\n",
    "      #    print(f\"Weighted average for each second: {weighted_average / iter_count}\")\n",
    "      #    print(f\"Predicted class for test audio: {final_class}, {classes_map[final_class]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          relative_path\n",
       "class_id               \n",
       "0                    81\n",
       "1                    80\n",
       "2                   164\n",
       "3                    80"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.groupby('class_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          relative_path\n",
       "class_id               \n",
       "0                    80\n",
       "1                    80\n",
       "2                    80\n",
       "3                    80"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = training_df.drop(training_df[training_df['class_id'] == 2].sample(n=84).index)\n",
    "training_df = training_df.drop(training_df[training_df['class_id'] == 0].sample(n=1).index)\n",
    "# training_df = training_df.drop(training_df[training_df['class_id'] == 1].sample(n=66).index)\n",
    "training_df.groupby('class_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training_set\\urination\\urination_1_0_quant_3.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training_set\\urination\\urination_1_10_quant_3.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training_set\\urination\\urination_1_11_quant_3.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training_set\\urination\\urination_1_12_quant_3.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training_set\\urination\\urination_1_2_quant_3.wav</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       relative_path  class_id\n",
       "0   training_set\\urination\\urination_1_0_quant_3.wav         2\n",
       "1  training_set\\urination\\urination_1_10_quant_3.wav         2\n",
       "2  training_set\\urination\\urination_1_11_quant_3.wav         2\n",
       "3  training_set\\urination\\urination_1_12_quant_3.wav         2\n",
       "4   training_set\\urination\\urination_1_2_quant_3.wav         2"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = training_df.reset_index(drop=True)\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files 256\n",
      "Number of validation files 64\n"
     ]
    }
   ],
   "source": [
    "training_ds = AudioDataSet(training_df, False)\n",
    "\n",
    "num_items = len(training_ds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "print(f\"Number of training files {num_train}\")\n",
    "print(f\"Number of validation files {num_val}\")\n",
    "\n",
    "train_ds, val_ds = random_split(training_ds, [num_train, num_val])\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_folder = 'testing_set'\n",
    "\n",
    "testing_path = Path.cwd()/testing_folder\n",
    "\n",
    "testing_data_dict = {'relative_path': [], 'class_id': []}\n",
    "\n",
    "test_class_name = 'defecation'\n",
    "path = f'{testing_path}/{test_class_name}'\n",
    "for file in os.listdir(path):\n",
    "    testing_data_dict['relative_path'].append(os.path.join(f'{testing_folder}\\\\{test_class_name}', file))\n",
    "    testing_data_dict['class_id'].append(99)\n",
    "                \n",
    "testing_df = pd.DataFrame(testing_data_dict)\n",
    "testing_ds = AudioDataSet(testing_df, True)\n",
    "test_dl = torch.utils.data.DataLoader(testing_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "Epoch: 0, Loss: 1.27, Accuracy: 0.44\n",
      "Epoch: 25, Loss: 0.85, Accuracy: 0.57\n",
      "Epoch: 50, Loss: 0.68, Accuracy: 0.70\n",
      "Epoch: 75, Loss: 0.66, Accuracy: 0.71\n",
      "Finished Training, total items 256\n",
      "\n",
      "Validation Set:\n",
      "Accuracy: 0.75, Total items: 16\n",
      "Accuracy: 0.75, Total items: 32\n",
      "Accuracy: 0.77, Total items: 48\n",
      "Accuracy: 0.72, Total items: 64\n",
      "\n",
      "Testing Set:\n",
      "Predictions for 16 seconds ['silence', 'defecation', 'urination', 'speech', 'speech', 'silence', 'defecation', 'defecation', 'defecation', 'urination', 'defecation', 'defecation', 'defecation', 'defecation', 'defecation', 'defecation']\n",
      "Predictions for 16 seconds ['defecation', 'speech', 'silence', 'speech', 'speech', 'urination', 'defecation', 'urination', 'defecation', 'urination', 'urination', 'defecation', 'defecation', 'urination', 'defecation', 'defecation']\n",
      "Predictions for 16 seconds ['silence', 'defecation', 'defecation', 'urination', 'silence', 'urination', 'silence', 'speech', 'silence', 'urination', 'urination', 'defecation', 'defecation', 'defecation', 'silence', 'silence']\n",
      "Predictions for 16 seconds ['defecation', 'urination', 'urination', 'defecation', 'silence', 'silence', 'urination', 'silence', 'defecation', 'urination', 'urination', 'silence', 'defecation', 'silence', 'silence', 'speech']\n",
      "Predictions for 16 seconds ['urination', 'defecation', 'silence', 'silence', 'urination', 'silence', 'defecation', 'silence', 'defecation', 'urination', 'urination', 'defecation', 'silence', 'urination', 'silence', 'speech']\n",
      "Predictions for 16 seconds ['speech', 'defecation', 'urination', 'speech', 'silence', 'silence', 'silence', 'defecation', 'silence', 'urination', 'silence', 'urination', 'silence', 'defecation', 'urination', 'silence']\n",
      "Predictions for 14 seconds ['silence', 'silence', 'silence', 'defecation', 'silence', 'silence', 'defecation', 'defecation', 'defecation', 'defecation', 'defecation', 'defecation', 'silence', 'silence']\n"
     ]
    }
   ],
   "source": [
    "num_epochs=100\n",
    "model = Classifier()\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Training Set:\")\n",
    "training(model, train_dl, num_epochs)\n",
    "print(\"\\nValidation Set:\")\n",
    "testing(model, val_dl, validation=True)\n",
    "print(\"\\nTesting Set:\")\n",
    "testing(model, test_dl, validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving = False\n",
    "if saving:\n",
    "    model_name = '1_bit_model'\n",
    "    torch.save(model.state_dict(), f'saved_models/{model_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
