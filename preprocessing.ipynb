{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "import soundfile as sf\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training_set\\pee\\number_one_quant_3_5.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training_set\\pee\\number_one_quant_3_6.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training_set\\poop\\number_two_quant_3_0.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training_set\\poop\\number_two_quant_3_2.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training_set\\poop\\number_two_quant_3_3.wav</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                relative_path  class_id\n",
       "0   training_set\\pee\\number_one_quant_3_5.wav         0\n",
       "1   training_set\\pee\\number_one_quant_3_6.wav         0\n",
       "2  training_set\\poop\\number_two_quant_3_0.wav         1\n",
       "3  training_set\\poop\\number_two_quant_3_2.wav         1\n",
       "4  training_set\\poop\\number_two_quant_3_3.wav         1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_folder = 'training_set'\n",
    "testing_folder = 'testing_set'\n",
    "\n",
    "training_path = Path.cwd()/training_folder\n",
    "testing_path = Path.cwd()/testing_folder\n",
    "folder_paths = [training_path, testing_path]\n",
    "\n",
    "classes = {'pee': 0, 'poop': 1, 'silence': 2, 'speech': 3}\n",
    "training_data_dict, testing_data_dict = {'relative_path': [], 'class_id': []}, {'relative_path': [], 'class_id': []}\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "    for class_name in classes:\n",
    "        path = f'{folder_path}/{class_name}'\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith('.wav'):\n",
    "                if folder_path == training_path:\n",
    "                    training_data_dict['relative_path'].append(os.path.join(f'{training_folder}\\\\{class_name}', file))\n",
    "                    training_data_dict['class_id'].append(classes[class_name])\n",
    "                else:\n",
    "                    testing_data_dict['relative_path'].append(os.path.join(f'{testing_folder}\\\\{class_name}', file))\n",
    "                    testing_data_dict['class_id'].append(classes[class_name])\n",
    "\n",
    "training_df = pd.DataFrame(training_data_dict)\n",
    "testing_df = pd.DataFrame(testing_data_dict)\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioUtil():\n",
    "  @staticmethod\n",
    "  def open(audio_file):\n",
    "    sig, sr = torchaudio.load(audio_file) #signal and sample rate\n",
    "    return (sig, sr)\n",
    "  \n",
    "  @staticmethod\n",
    "  def plot_waveform(waveform, sample_rate):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "    figure.suptitle(\"waveform\")\n",
    "\n",
    "  @staticmethod\n",
    "  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return (spec)\n",
    "  \n",
    "  @staticmethod\n",
    "  def pad_trunc(aud, max_ms):\n",
    "    sig, sr = aud\n",
    "    num_rows, sig_len = sig.shape\n",
    "    max_len = sr//1000 * max_ms\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "      sig = sig[:,:max_len]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "      pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)\n",
    "  \n",
    "  @staticmethod\n",
    "  def resample(aud, newsr):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sr == newsr):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    num_channels = sig.shape[0]\n",
    "    # Resample first channel\n",
    "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "    if (num_channels > 1):\n",
    "      # Resample the second channel and merge both channels\n",
    "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "      resig = torch.cat([resig, retwo])\n",
    "\n",
    "    return ((resig, newsr))\n",
    "  \n",
    "  @staticmethod\n",
    "  def rechannel(aud, new_channel):\n",
    "    sig, sr = aud\n",
    "\n",
    "    if (sig.shape[0] == new_channel):\n",
    "      # Nothing to do\n",
    "      return aud\n",
    "\n",
    "    if (new_channel == 1):\n",
    "      # Convert from stereo to mono by selecting only the first channel\n",
    "      resig = sig[:1, :]\n",
    "    else:\n",
    "      # Convert from mono to stereo by duplicating the first channel\n",
    "      resig = torch.cat([sig, sig])\n",
    "\n",
    "    return ((resig, sr))\n",
    "  \n",
    "  @staticmethod\n",
    "  def time_shift(aud, shift_limit):\n",
    "    sig,sr = aud\n",
    "    _, sig_len = sig.shape\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    return (sig.roll(shift_amt), sr)\n",
    "  \n",
    "  @staticmethod\n",
    "  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataSet(Dataset):\n",
    "  def __init__(self, df):\n",
    "    self.df = df\n",
    "    self.duration = 4000\n",
    "    self.channel = 2\n",
    "    self.sr = 48000\n",
    "    self.shift_pct = 0.4\n",
    "            \n",
    "  def __len__(self):\n",
    "    return len(self.df)    \n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    audio_file = self.df.loc[idx, 'relative_path']\n",
    "    class_id = self.df.loc[idx, 'class_id']\n",
    "\n",
    "    aud = AudioUtil.open(audio_file)\n",
    "    reaud = AudioUtil.resample(aud, self.sr)\n",
    "    rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "\n",
    "    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "    return aug_sgram, class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = torch.nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.bn1 = torch.nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Convolution Block\n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.bn2 = torch.nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Convolution Block\n",
    "        self.conv3 = torch.nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        self.bn3 = torch.nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Convolution Block\n",
    "        self.conv4 = torch.nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "        self.bn4 = torch.nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = torch.nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = torch.nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = torch.nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_dl, num_epochs):\n",
    "  # Loss Function, Optimizer and Scheduler\n",
    "  criterion = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "  # Repeat for each epoch\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Repeat for each batch in the training set\n",
    "    for i, data in enumerate(train_dl):\n",
    "        inputs, labels = data[0], data[1]\n",
    "\n",
    "        # Normalize the inputs\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Keep stats for Loss and Accuracy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Get the predicted class with the highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        # Count of predictions that matched the target label\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "\n",
    "        #if i % 10 == 0:    # print every 10 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "    \n",
    "    # Print stats at the end of the epoch\n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    if epoch % 50 == 0:\n",
    "       print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "  print(f'Finished Training, total items {total_prediction}')\n",
    "\n",
    "def testing(model, val_dl, validation):\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Disable gradient updates\n",
    "    with torch.no_grad():\n",
    "      for data in val_dl:\n",
    "        inputs, labels = data[0], data[1]\n",
    "\n",
    "        # Normalization\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Get predictions\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get class with highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        if validation:\n",
    "           correct_prediction += (prediction == labels).sum().item()\n",
    "           total_prediction += prediction.shape[0]\n",
    "           acc = correct_prediction/total_prediction\n",
    "           print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "        else:\n",
    "           print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ds = AudioDataSet(training_df)\n",
    "testing_ds = AudioDataSet(testing_df)\n",
    "\n",
    "num_items = len(training_ds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "\n",
    "train_ds, val_ds = random_split(training_ds, [num_train, num_val])\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "test_dl = torch.utils.data.DataLoader(testing_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "Epoch: 0, Loss: 2.37, Accuracy: 0.07\n",
      "Epoch: 50, Loss: 1.35, Accuracy: 0.64\n",
      "Finished Training, total items 14\n",
      "\n",
      "Validation Set:\n",
      "Accuracy: 0.75, Total items: 4\n",
      "\n",
      "Testing Set:\n",
      "tensor([3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2])\n",
      "tensor([2, 0, 0, 3, 3, 2, 2, 3, 2, 1, 2, 3, 2, 2, 2, 2])\n",
      "tensor([3, 2, 2, 2, 3, 2, 2, 3, 0, 2, 2, 2, 2, 2, 3, 0])\n",
      "tensor([2, 2, 1, 2, 2, 0, 3, 2, 3, 2, 2, 3, 3, 3, 3, 2])\n",
      "tensor([2, 2, 3, 2, 2, 3, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2])\n",
      "tensor([0, 2, 2, 3, 3, 1, 3, 0, 2, 3, 3, 2, 2, 2, 2, 3])\n",
      "tensor([3, 0, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "num_epochs=100\n",
    "model = Classifier()\n",
    "\n",
    "print(\"Training Set:\")\n",
    "training(model, train_dl, num_epochs)\n",
    "print(\"\\nValidation Set:\")\n",
    "testing(model, val_dl, True)\n",
    "print(\"\\nTesting Set:\")\n",
    "testing(model, test_dl, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
